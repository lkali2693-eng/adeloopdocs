---
title: Ingesting Data
description: How to ingest CSV/Excel, internal and external databases into Adeloop notebooks and register datasets for reuse.
icon: Database
---

import ToolsIntegration from '../../../components/ui/tools-integration'


<ToolsIntegration />

This short guide explains common ways to bring data into Adeloop so it can be used in notebooks or registered as datasets for other pages. It covers:

- Internal file formats (CSV, Excel)
- Relational databases commonly hosted internally (Postgres, MySQL) comming soon,
- Document stores (MongoDB) comming soon
- External analytic warehouses (Snowflake) comming soon

Security note: never store credentials in source control. Use environment variables or secrets management.


![Data Ingestion](/data_ingest.svg)

## Data Ingestion Workflow

```mermaid
flowchart TD;
  CSV["Import CSV / Excel files"];
  NOTEBOOK["Analyze in Jupyter-like notebook"];
  AI["AI assists: cleaning, EDA, modeling"];
  GENIDE["Auto-generate dashboards (GenIDE)"];
  SHARE["Share insights instantly"];

  CSV --> NOTEBOOK;
  NOTEBOOK --> AI;
  AI --> GENIDE;
  GENIDE --> SHARE;
```

### External sources (metadata-only integration)

The platform connects to external databases using connectors that fetch data on-demand. Adeloop stores only connection metadata (endpoint, credentials, schema info, table lists, versions) — it does NOT persist external data in its own database. Notebooks request data through the platform's connector API which retrieves, streams, or previews rows directly from the source at query-time.

```mermaid
flowchart LR
  UI[User / Notebook UI];
  API[Adeloop Connector Service];
  Metadata[Metadata Store];
  Notebook[Notebook Runtime];
  subgraph ExternalSources [External Databases];
    PG[Postgres];
    MY[MySQL];
    MG[MongoDB];
  end;

  UI -->|configure connection| API;
  API -->|save connection metadata| Metadata;
  Notebook -->|select dataset / query| API;
  API -->|on-demand fetch - no storage| PG;
  API -->|on-demand fetch - no storage| MY;
  API -->|on-demand fetch - no storage| MG;
  API -->|return rows / stream| Notebook;
  Metadata -.->|used for discovery & schema| UI;
```

### User Journey in Data Ingestion

```mermaid
journey
    title Data Ingestion User Journey
    section Discovery
      Find data source: 5: User
      Review source docs: 4: User
    section Setup
      Configure credentials: 3: User, DataEngineer
      Test connection: 4: DataEngineer
      Validate schema: 5: DataEngineer
    section Integration
      Ingest data: 5: System
      Register dataset: 5: User
      Create version: 5: System
    section Verification
      Run quality checks: 4: System
      Approve for use: 5: User
```

---

## 1) CSV & Excel (local / internal files)

These are the simplest and work well inside notebooks.

### CSV/Excel Ingestion Flow


Python (pandas) examples you can run in a notebook:

```python
import pandas as pd

# CSV
df = pd.read_csv("/path/to/data.csv")

# Excel (first sheet)
df_x = pd.read_excel("/path/to/data.xlsx", sheet_name=0)

# Quick preview
print(df.head())
```

Notes and tips:
- For large CSVs prefer reading with `chunksize=` or converting to Parquet for faster reuse: `df.to_parquet("data/mydata.parquet")`.
- If users upload files via the UI, place them in a data directory the notebook can reach (for example a mounted `data/` folder). Avoid committing large data files to the repo.

---

## 2) Relational databases (Postgres, MySQL)

Adeloop supports connecting to relational databases (Postgres, MySQL) via managed connectors. You register a connection in the UI (host, port, user, secret stored securely) and the platform saves only the connection metadata and schema/catalog information. When you open a notebook you can select a table or run an inspected query — the platform fetches rows on-demand and returns them to the notebook runtime. This means you do not need to install database drivers like `psycopg2` or `mysql-connector` inside the notebook environment for exploration and previewing.

If you prefer to connect directly from a notebook (self-managed), you can still use standard drivers; however the recommended pattern inside Adeloop notebooks is to call the platform's data API or helper instead, for example (conceptual):

```python
# Conceptual example using the Adeloop connector API (no DB drivers required)
from adeloop import data

# 'my_postgres' is the connection name created in the Adeloop UI
df = data.query("my_postgres", "SELECT * FROM my_schema.my_table LIMIT 1000")
print(df.head())
```

The code above is pseudocode showing the pattern: select a registered connection by name and run a query through the platform. The platform handles the secure connection, streaming, and serialization back into a DataFrame-like object in the notebook.

Use a DB driver plus SQLAlchemy for a consistent interface. Provide credentials via environment variables.

### Database Connection Architecture


Install required packages (example):

```bash
pip install pandas sqlalchemy psycopg2-binary mysql-connector-python
```

Example: Postgres

```python
import os
import pandas as pd
from sqlalchemy import create_engine

PG_USER = os.getenv("PG_USER")
PG_PASS = os.getenv("PG_PASS")
PG_HOST = os.getenv("PG_HOST")
PG_PORT = os.getenv("PG_PORT", "5432")
PG_DB = os.getenv("PG_DB")

engine = create_engine(f"postgresql+psycopg2://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}")

query = "SELECT * FROM my_schema.my_table LIMIT 1000"
df = pd.read_sql(query, engine)
```

Example: MySQL

```python
from sqlalchemy import create_engine

engine = create_engine("mysql+mysqlconnector://user:password@host:3306/dbname")
df = pd.read_sql("SELECT * FROM some_table LIMIT 500", engine)
```

Tips:
- Avoid SELECT * on large tables; limit and filter in SQL for performance.
- Use connection pooling and short-lived credentials if possible.

---

## 3) MongoDB (document store)

Adeloop also supports document stores like MongoDB through the same connector model. Register your MongoDB connection in the UI (connection string, auth details), and the system will index available collections and sample schemas as metadata. In notebooks you can select a collection or issue filtered queries via the platform API — no need to install `pymongo` in the notebook to preview or analyze documents.

Conceptual example using the platform helper (no pymongo required):

```python
# Conceptual API - platform handles connection and streaming
from adeloop import data

# select the collection by connection name and collection id
df = data.select_collection("my_mongo", "my_collection", limit=500)
print(df.head())
```

Notes:
- Documents can contain nested structures; the platform will return flattened rows or JSON blobs depending on the selected preview option. Use `pd.json_normalize` locally if you need specific normalization for complex nested documents.
- For very large collections, the connector streams results and the notebook can iterate in batches.

### NoSQL Document Pipeline

Install the driver:

```bash
pip install pymongo
```

Example using `pymongo` and converting to a pandas DataFrame:

```python
from pymongo import MongoClient
import pandas as pd
import os

MONGO_URI = os.getenv("MONGO_URI")  # mongodb+srv://user:pass@host
client = MongoClient(MONGO_URI)

db = client.get_database("mydb")
collection = db.get_collection("my_collection")

cursor = collection.find({}, limit=1000)
records = list(cursor)

df = pd.DataFrame.from_records(records)
```

Notes:
- Documents can contain nested structures; normalize with `pd.json_normalize` if needed.
- For very large collections, stream results and process in chunks.

---

## 4) Snowflake (external analytic warehouse)

Snowflake provides a fast way to access large analytical datasets. Use the Snowflake Python connector or SQLAlchemy dialect.

### Cloud Data Warehouse Flow

Install packages:

```bash
pip install "snowflake-connector-python[pandas]" sqlalchemy snowflake-sqlalchemy
```

Example using SQLAlchemy engine:

```python
import os
import pandas as pd
from sqlalchemy import create_engine

SF_USER = os.getenv("SF_USER")
SF_PASS = os.getenv("SF_PASS")
SF_ACCOUNT = os.getenv("SF_ACCOUNT")
SF_WAREHOUSE = os.getenv("SF_WAREHOUSE")
SF_DATABASE = os.getenv("SF_DATABASE")
SF_SCHEMA = os.getenv("SF_SCHEMA")

engine = create_engine(
    f"snowflake://{SF_USER}:{SF_PASS}@{SF_ACCOUNT}/{SF_DATABASE}/{SF_SCHEMA}?warehouse={SF_WAREHOUSE}"
)

sql = "SELECT * FROM ANALYTICS.LOOKS LIMIT 1000"
df = pd.read_sql(sql, engine)
```

Notes:
- Credentials should come from secure stores or environment variables.
- For repeated use, consider creating a view or a smaller extracted dataset.


---

## 6) Best practices & security

### Data Governance & Security Flow


- Never hardcode credentials; use environment vars or secrets.
- For production workloads, push heavy lifting (ingest, transformation) to jobs rather than interactive notebooks.
- Log data provenance: where the data came from, which query was used, and when it was refreshed.
- Use limited-scope credentials and network controls.

---
also in dataeditor we have CSheet

![Adeloop Platform](/data_version.svg)

## CSheet — in-place dataset editing and GitHub-style versioning

Adeloop supports a spreadsheet-like editor (CSheet) that lets users open a dataset (CSV/Excel), edit rows and columns interactively, and save changes as a versioned dataset — similar to saving a file in GitHub with a commit history.

Key behaviors:
- Edit in place: users can perform small edits, corrections, and simple transforms directly in the UI.
- Versioned saves: when saving, CSheet can create an immutable snapshot (a new dataset version) with metadata (author, message, timestamp). This preserves history and makes rollbacks possible.
- GitHub-style push: optionally, the platform can persist a copy of the dataset to a Git repository (or create a pull request) using an API token stored in secrets.

Conceptual example (not run here) showing how a notebook might export an edited CSV and push it to a repo using a GitHub token kept in an environment variable:

```python
# export the edited dataset from the notebook workspace
df.to_csv("/workspace/data/mydataset.csv", index=False)

# push to GitHub (conceptual example; use secure tokens and small files)
from github import Github
import os

G = Github(os.getenv("GITHUB_TOKEN"))
repo = G.get_repo("owner/repo")
path = "data/mydataset.csv"
content = open("/workspace/data/mydataset.csv", "r", encoding="utf-8").read()
try:
    existing = repo.get_contents(path)
    repo.update_file(path, "Update dataset via CSheet", content, existing.sha)
except Exception:
    repo.create_file(path, "Add dataset via CSheet", content)
```

Notes and recommendations:
- Prefer storing large datasets in object storage (S3, GCS, internal file share) and keep Git for small CSVs or metadata only.
- Do not commit secrets; use the platform's secrets store or environment variables for API tokens.
- When integrating with GitHub, consider creating a pull request rather than directly overwriting production files so changes can be reviewed.
- After saving a versioned dataset, register it in the dataset catalog (path or dataset ID) so other notebooks and pages can reference it reliably.

If you'd like, I can add:
- a small helper module that wraps save/push operations safely (e.g., `lib/data_connectors.py`), or
- a short example notebook showing editable CSheet flow and pushing a small CSV to a repo.
